{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LLM Ticket Triage- Google Colab\n",
                "\n",
                "**Repo:** [https://github.com/Kangarroar/LLM-Ticket-Triage](https://github.com/Kangarroar/LLM-Ticket-Triage)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment & Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Clone Repository & Install Dependencies\n",
                "import os\n",
                "\n",
                "# Clone repo\n",
                "if not os.path.exists(\"LLM-Ticket-Triage\"):\n",
                "    !git clone https://github.com/Kangarroar/LLM-Ticket-Triage\n",
                "else:\n",
                "    %cd LLM-Ticket-Triage\n",
                "    !git pull\n",
                "    %cd ..\n",
                "\n",
                "# Enter repo directory\n",
                "%cd LLM-Ticket-Triage\n",
                "\n",
                "# Install dependencies from requirements.txt\n",
                "!pip install -r requirements.txt\n",
                "\n",
                "# Verify GPU\n",
                "import torch\n",
                "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Dataset (Optional)\n",
                "Run this if you need to create synthetic data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Run Synthetic Data Generator\n",
                "Generate_Data = True #@param {type:\"boolean\"}\n",
                "\n",
                "if Generate_Data:\n",
                "    !python training/synthetic_data_generation.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Configuration\n",
                "Configure your training parameters using the dropdowns below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Configuration Parameters\n",
                "import yaml\n",
                "\n",
                "# Model Config\n",
                "model_name = \"Qwen/Qwen2.5-1.5B\" #@param [\"Qwen/Qwen2.5-1.5B\", \"Qwen/Qwen2.5-0.5B\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"]\n",
                "max_length = 512 #@param {type:\"slider\", min:128, max:512, step:64}\n",
                "\n",
                "# LoRA Config\n",
                "lora_r = 16 #@param [8, 16, 32, 64]\n",
                "lora_alpha = 32 #@param [16, 32, 64]\n",
                "lora_dropout = 0.05 #@param {type:\"number\"}\n",
                "\n",
                "# Training Hyperparameters\n",
                "num_epochs = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
                "batch_size = 4 #@param [1, 2, 4, 8, 16]\n",
                "eval_batch_size = 4 #@param [1, 2, 4, 8]\n",
                "gradient_accumulation_steps = 4 #@param [1, 2, 4, 8, 16]\n",
                "learning_rate = 2e-4 #@param {type:\"number\"}\n",
                "warmup_steps = 100 #@param {type:\"number\"}\n",
                "weight_decay = 0.01 #@param {type:\"number\"}\n",
                "\n",
                "# Logging & Evaluation\n",
                "logging_steps = 10 #@param {type:\"number\"}\n",
                "eval_steps = 100 #@param {type:\"number\"}\n",
                "save_steps = 100 #@param {type:\"number\"}\n",
                "\n",
                "# Precision\n",
                "precision = \"fp16\" #@param [\"fp32\", \"fp16\", \"bf16\"]\n",
                "\n",
                "# Early Stopping\n",
                "early_stopping_patience = 3 #@param {type:\"integer\"}\n",
                "early_stopping_threshold = 0.001 #@param {type:\"number\"}\n",
                "\n",
                "\n",
                "# Construct Dictionary\n",
                "config = {\n",
                "    \"model\": {\n",
                "        \"name\": model_name,\n",
                "        \"max_length\": max_length\n",
                "    },\n",
                "    \"lora\": {\n",
                "        \"r\": lora_r,\n",
                "        \"lora_alpha\": lora_alpha,\n",
                "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
                "        \"lora_dropout\": lora_dropout,\n",
                "        \"bias\": \"none\",\n",
                "        \"task_type\": \"CAUSAL_LM\"\n",
                "    },\n",
                "    \"quantization\": {\n",
                "        \"load_in_4bit\": True,\n",
                "        \"compute_dtype\": \"float16\",\n",
                "        \"quant_type\": \"nf4\",\n",
                "        \"use_double_quant\": True\n",
                "    },\n",
                "    \"training\": {\n",
                "        \"output_dir\": \"./models/adapters/qwen_1.5b_it_tickets\",\n",
                "        \"prediction_loss_only\": True, #If false easy to OOM.\n",
                "        \"num_train_epochs\": num_epochs,\n",
                "        \"per_device_train_batch_size\": batch_size,\n",
                "        \"per_device_eval_batch_size\": eval_batch_size,\n",
                "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
                "        \"learning_rate\": learning_rate,\n",
                "        \"lr_scheduler_type\": \"cosine\",\n",
                "        \"warmup_steps\": warmup_steps,\n",
                "        \"weight_decay\": weight_decay,\n",
                "        \n",
                "        \"logging_steps\": logging_steps,\n",
                "        \"logging_first_step\": True,\n",
                "        \"log_level\": \"info\",\n",
                "        \"eval_strategy\": \"steps\",\n",
                "        \"eval_steps\": eval_steps,\n",
                "        \"eval_accumulation_steps\": 1,\n",
                "        \"save_strategy\": \"steps\",\n",
                "        \"save_steps\": save_steps,\n",
                "        \"save_total_limit\": 3,\n",
                "        \n",
                "        \"precision\": precision,\n",
                "        \"optim\": \"adamw_torch\",\n",
                "        \"gradient_checkpointing\": True,\n",
                "        \"max_grad_norm\": 1.0,\n",
                "        \n",
                "        \"seed\": 42,\n",
                "        \"load_best_model_at_end\": True,\n",
                "        \"metric_for_best_model\": \"eval_loss\",\n",
                "        \"greater_is_better\": False,\n",
                "        \"remove_unused_columns\": False,\n",
                "        \"disable_tqdm\": False,\n",
                "        \n",
                "        \"report_to\": \"tensorboard\",\n",
                "        \"logging_dir\": \"./logs/tensorboard\"\n",
                "    },\n",
                "    \"early_stopping\": {\n",
                "        \"patience\": early_stopping_patience,\n",
                "        \"threshold\": early_stopping_threshold\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save to yaml\n",
                "with open(\"configs/training_config.yaml\", \"w\") as f:\n",
                "    yaml.dump(config, f)\n",
                "\n",
                "print(\"Configuration saved to configs/training_config.yaml\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Launch TensorBoard\n",
                "%load_ext tensorboard\n",
                "%tensorboard --logdir ./logs/tensorboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Start Training\n",
                "!python training/train_lora.py"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Inference Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "from peft import PeftModel\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "model_name = \"Qwen/Qwen2.5-1.5B\"\n",
                "adapter_path = \"./models/adapters/qwen_1.5b_it_tickets_final\"  #@param {type:\"string\"}\n",
                "\n",
                "# schema\n",
                "CATEGORIES = [\"Hardware\", \"Software\", \"Access\", \"Network\", \"Security\", \"Workplace\"]\n",
                "SUBCATEGORIES = [\"Login Issues\", \"Permissions\", \"Malware\", \"Physical Security\", \"Policy Violation\", \"Other\"]\n",
                "PRIORITIES = [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
                "ASSIGNMENT_GROUPS = [\"End User Applications\", \"Network Operations\", \"Desktop Support\", \"Email / Messaging\", \"Security / Access\", \"Hardware Support\", \"Facilities\"]\n",
                "\n",
                "instruction = \"\"\"You MUST output valid JSON matching this schema:\n",
                "{\n",
                "  \"summary\": string,\n",
                "  \"category\": one of %s,\n",
                "  \"subcategory\": one of %s,\n",
                "  \"priority\": one of %s,\n",
                "  \"assignment_group\": one of %s,\n",
                "  \"request_type\": \"Incident\" | \"Service Request\"\n",
                "}\n",
                "\n",
                "No extra keys. No explanations.\"\"\" % (json.dumps(CATEGORIES), json.dumps(SUBCATEGORIES), json.dumps(PRIORITIES), json.dumps(ASSIGNMENT_GROUPS))\n",
                "\n",
                "user_input = \"fire on server place\"  #@param {type:\"string\"}\n",
                "\n",
                "max_new_tokens = 196  #@param {type:\"integer\"}\n",
                "use_cache=True\n",
                "use_sampling = False  #@param {type:\"boolean\"}\n",
                "\n",
                "print(f\"Loading tokenizer: {model_name}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    model_name,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float32,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    device_map={\"\": 0},\n",
                "    quantization_config=bnb_config,\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "model = PeftModel.from_pretrained(model, adapter_path)\n",
                "model.eval()\n",
                "\n",
                "prompt = f\"\"\"### Instruction:\n",
                "{instruction}\n",
                "\n",
                "### Input:\n",
                "{user_input}\n",
                "\n",
                "### Output:\n",
                "\"\"\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=max_new_tokens,\n",
                "        do_sample=use_sampling,\n",
                "        temperature=0.7 if use_sampling else 0.0,\n",
                "        eos_token_id=tokenizer.eos_token_id,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"\\n===== MODEL OUTPUT =====\\n\")\n",
                "print(result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Option A: Download Adapter (Zip)\n",
                "import shutil\n",
                "from google.colab import files\n",
                "\n",
                "shutil.make_archive(\"adapter_model\", 'zip', \"./models/adapters/qwen_1.5b_it_tickets_final\")\n",
                "files.download(\"adapter_model.zip\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title Option B: Save to Google Drive\n",
                "from google.colab import drive\n",
                "import shutil\n",
                "import os\n",
                "\n",
                "# Mount Drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "drive_path = \"/content/drive/MyDrive/LLM_Adapters/qwen_1.5b_it_tickets_final\" #@param {type:\"string\"}\n",
                "\n",
                "if os.path.exists(\"./models/adapters/qwen_1.5b_it_tickets_final\"):\n",
                "    # Create destination directory if it doesn't exist\n",
                "    os.makedirs(drive_path, exist_ok=True)\n",
                "    \n",
                "    # Copy files\n",
                "    src_dir = \"./models/adapters/qwen_1.5b_it_tickets_final\"\n",
                "    for item in os.listdir(src_dir):\n",
                "        s = os.path.join(src_dir, item)\n",
                "        d = os.path.join(drive_path, item)\n",
                "        if os.path.isdir(s):\n",
                "            shutil.copytree(s, d, dirs_exist_ok=True)\n",
                "        else:\n",
                "            shutil.copy2(s, d)\n",
                "    \n",
                "    print(f\"Adapter saved to {drive_path}\")\n",
                "else:\n",
                "    print(\"Adapter not found\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
