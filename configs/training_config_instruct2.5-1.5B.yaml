early_stopping:
  patience: 3
  threshold: 0.001
lora:
  bias: none
  lora_alpha: 32
  lora_dropout: 0.05
  r: 16
  target_modules:
  - q_proj
  - v_proj
  task_type: CAUSAL_LM
model:
  max_length: 192
  name: Qwen/Qwen2.5-1.5B-Instruct
quantization:
  compute_dtype: float16
  load_in_4bit: true
  quant_type: nf4
  use_double_quant: true
training:
  disable_tqdm: false
  eval_accumulation_steps: 1
  eval_steps: 100
  eval_strategy: steps
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  greater_is_better: false
  learning_rate: 8.0e-05
  load_best_model_at_end: true
  log_level: info
  logging_dir: ./logs/tensorboard
  logging_first_step: true
  logging_steps: 10
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  metric_for_best_model: eval_loss
  num_train_epochs: 3
  optim: adamw_torch
  output_dir: ./models/adapters/qwen_1.5b_it_tickets
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 4
  precision: fp16
  prediction_loss_only: true
  remove_unused_columns: false
  report_to: tensorboard
  save_steps: 100
  save_strategy: steps
  save_total_limit: 3
  seed: 42
  warmup_steps: 30
  weight_decay: 0.01
